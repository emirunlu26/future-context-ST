{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emirunlu26/future-context-ST/blob/main/code/future_context_ST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnDTaWvyLnWU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2-J65W0LLwg"
      },
      "source": [
        "**READ THE REFERENCE TRANSCRIPTIONS OF EUROPARL**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_samples = 500"
      ],
      "metadata": {
        "id": "waz0lx9BWoEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO-7VI-3LJot"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/ColabNotebooks/segments.en\",\"r\") as transcr_file:\n",
        "  ref_transcriptions = list()\n",
        "  for transcription in transcr_file:\n",
        "    transcription = transcription[:len(transcription)-1]\n",
        "    ref_transcriptions.append(transcription)\n",
        "    if len(ref_transcriptions) == num_of_samples:\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_predictions = 0\n",
        "\n",
        "for ref_sentence in ref_transcriptions:\n",
        "  words_in_ref = ref_sentence.split()\n",
        "  num_of_words_in_ref = len(words_in_ref)\n",
        "  total_predictions += num_of_words_in_ref - 1\n",
        "\n",
        "print(total_predictions)"
      ],
      "metadata": {
        "id": "FgPytxV8WrdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/ColabNotebooks/SimulEval/Whisper-Wait-k/asr_target_test_set.txt\",\"r\") as transcr_file:\n",
        "  ref_transcriptions = list()\n",
        "  for transcription in transcr_file:\n",
        "    transcription = transcription[:len(transcription)-1]\n",
        "    ref_transcriptions.append(transcription)"
      ],
      "metadata": {
        "id": "N1HnqGiFqOYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COUNT NUMBER OF SENTENCES GROUPED BY NUMBER OF WORDS\n",
        "\n",
        "def get_group_index(num_of_words):\n",
        "  INTERVAL_SIZE = 5\n",
        "\n",
        "  div = int(num_of_words / INTERVAL_SIZE)\n",
        "  modulo = num_of_words % INTERVAL_SIZE\n",
        "\n",
        "  if modulo == 0:\n",
        "    return div - 1\n",
        "  else:\n",
        "    return div\n",
        "\n",
        "intervals = [5,10,15,20,25,30,35,40,45,50,55,60,65]\n",
        "size_of_intervals = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "for ref_sentence in ref_transcriptions:\n",
        "  words_in_ref = ref_sentence.split()\n",
        "  num_of_words_in_ref = len(words_in_ref)\n",
        "  group_index = get_group_index(num_of_words_in_ref)\n",
        "  size_of_intervals[group_index] += 1\n",
        "\n",
        "start_number = 1\n",
        "INTERVAL_SIZE = 5\n",
        "for index, interval_size in enumerate(size_of_intervals):\n",
        "  interval_name = str(start_number + index * INTERVAL_SIZE) + \"-\" + str(start_number + (((index+1) * INTERVAL_SIZE)-1))\n",
        "  print(\"Interval \" + interval_name + \": \" + str(interval_size))"
      ],
      "metadata": {
        "id": "uf-QpJbbqnNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_predictions_sum = 0\n",
        "for ref_sentence in ref_transcriptions:\n",
        "  words_in_ref = ref_sentence.split()\n",
        "  num_of_words_in_ref = len(words_in_ref)\n",
        "  num_of_word_predictions = num_of_words_in_ref - 1\n",
        "  word_predictions_sum += num_of_word_predictions\n",
        "\n",
        "print(\"Total Word Predictions:\", word_predictions_sum)"
      ],
      "metadata": {
        "id": "tM7FxtHBPdCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxVx_BCbG7p_"
      },
      "source": [
        "**DEFINE CLASS FOR FUTURE CONTEXT PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLZmTx7Ra836"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class QwenFutureContextPredictor():\n",
        "\n",
        "  def __init__(self, model_name, device, system_prompt, is_instruction_tuned, instruction_prefix = None, examples = None):\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=device)\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.system_prompt = system_prompt\n",
        "    self.is_instruction_tuned = is_instruction_tuned\n",
        "    self.instruction_prefix = instruction_prefix\n",
        "    self.examples = examples\n",
        "\n",
        "  def predict(self, partial_text, max_new_tokens = 1, with_examples = False):\n",
        "    user_prompt = self.instruction_prefix + \" \" + partial_text if self.is_instruction_tuned else partial_text\n",
        "    model_inputs = self.tokenizer([user_prompt], return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "    output_dict = self.model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True\n",
        "    )\n",
        "\n",
        "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, output_dict[\"sequences\"])]\n",
        "    prediction = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return (prediction,output_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqgPiOVfOaxY"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", torch_dtype=\"auto\", device_map=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1aPmlHEOqjc"
      },
      "outputs": [],
      "source": [
        "txt = \"This can be incomprhensible for\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "\n",
        "output = model.generate(\n",
        "    **tokenizer([txt], return_tensors=\"pt\").to(model.device),\n",
        "    max_new_tokens=1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzlq-NhiO1z-"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.batch_decode(output['sequences']), output['sequences'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRSvLklAuaV0"
      },
      "outputs": [],
      "source": [
        "for seq in output['sequences']:\n",
        "  print(\"Tensor:\", seq)\n",
        "  print(tokenizer.batch_decode(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5GQUsRAPS4Q"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(' is')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RCZekiBPaYv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.exp(torch.nn.functional.log_softmax(output[\"scores\"][0][0], dim=-1)[374])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfwSj_mTaiDN"
      },
      "source": [
        "**USE QWEN 2.5-0.5B-Instruct TO IMPLEMENT NEXT-WORD PREDICTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrPJOc95MThR"
      },
      "source": [
        "**FUTURE CONTEXT PREDICTION WITHOUT PREVIOUS CONTEXT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T6zxTT2NiK-"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    (\"You must answer the European citizens as to why\", \"they\"),\n",
        "    (\"I entirely agree\", \"with\"),\n",
        "    (\"There cannot be a return to the status quo ante, and I\", \"think\"),\n",
        "    (\"The Commission came up with a very carefully crafted position on third countries investing in the EU, suggesting\", \"that\"),\n",
        "    (\"The answer, we are repeatedly told,\", \"is\"),\n",
        "]\n",
        "\n",
        "# INSTANTIATE AN OBJECT FROM QwenFutureContextPredictor CLASS\n",
        "qwen_predictor = QwenFutureContextPredictor(\n",
        "    model_name= \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    device= \"cuda\",\n",
        "    system_prompt= \"Your mission is the next-token prediction for a given incomplete sentence.\",\n",
        "    is_instruction_tuned= True,\n",
        "    instruction_prefix= \"Predict the next token for the incomplete sentence:\",\n",
        "    examples = examples\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg2Z1wkLkdAF"
      },
      "outputs": [],
      "source": [
        "example_input = \"My dad wants me to\"\n",
        "prediction,output_dict = qwen_predictor.predict(example_input, max_new_tokens=1, with_examples=True)\n",
        "print(\"\\nPrediction:\",prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3MtVCbAUD8_"
      },
      "outputs": [],
      "source": [
        "input = qwen_predictor.tokenizer([ref_transcriptions[0]], return_tensors=\"pt\").to(qwen_predictor.model.device)\n",
        "print(input[\"input_ids\"][0])\n",
        "token_num = len(input[\"input_ids\"][0])\n",
        "print(\"Token num:\", token_num)\n",
        "num_of_tokens_to_read = round(token_num * 0.5)\n",
        "print(input[\"input_ids\"][0][:num_of_tokens_to_read])\n",
        "\n",
        "partial_sentence = qwen_predictor.tokenizer.decode(input[\"input_ids\"][0][:num_of_tokens_to_read + 1])\n",
        "print(\"Partial sentence:\", partial_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SezEVNezseS"
      },
      "outputs": [],
      "source": [
        "# Take a specified amount of words from reference sentences as input at each step\n",
        "\n",
        "initial_word_amount_to_read = 5 # amount of words in the input for the first step\n",
        "last_word_amount_to_read = 25  # amount of words in the input for the last step\n",
        "word_amount_step = 5  # number by which the amount of words in the input increases\n",
        "num_of_tokens_to_predict = 8 # number of tokens which should be predicted by the model\n",
        "with_examples = True\n",
        "\n",
        "final_result_list = list() # store results of reference sentences as a list of list where each element is a list of results for a specific reference sentence\n",
        "\n",
        "for ref_sentence in ref_transcriptions:\n",
        "  result_list = list() # store each result for a specific reference sentence as elements of a list\n",
        "  word_amount_to_read = initial_word_amount_to_read\n",
        "\n",
        "  while word_amount_to_read <= last_word_amount_to_read:\n",
        "    input_sentence = \" \".join(ref_sentence.split()[:word_amount_to_read])\n",
        "    print(\"Input sentence:\", input_sentence)\n",
        "    prediction, output_dict = qwen_predictor.predict(input_sentence, max_new_tokens=num_of_tokens_to_predict, with_examples=with_examples)\n",
        "    predicted_words = prediction.split()\n",
        "    print(\"Predicted text:\", prediction)\n",
        "    predicted_token_ids = qwen_predictor.tokenizer([prediction], return_tensors=\"pt\").to(qwen_predictor.model.device)[\"input_ids\"][0]\n",
        "    result_dict = {\n",
        "        \"word_amount_to_read\": word_amount_to_read,\n",
        "        \"predicted_token_ids\": predicted_token_ids,\n",
        "        \"predicted_words\": predicted_words,\n",
        "        \"input\": input_sentence,\n",
        "        \"ground-truth_words\": ref_sentence.split()[word_amount_to_read:word_amount_to_read + len(predicted_words)],\n",
        "        \"output_scores\": output_dict[\"scores\"]\n",
        "        }\n",
        "\n",
        "    print(\"Result:\", result_dict)\n",
        "    print()\n",
        "    result_list.append(result_dict)\n",
        "    word_amount_to_read += word_amount_step\n",
        "\n",
        "  final_result_list.append(result_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E6vdpA5MFn6"
      },
      "source": [
        "**FUTURE CONTEXT PREDICTION WITH PREVIOUS CONTEXT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovSG7vJQMMKR"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    (\"You must answer the European citizens as to why\", \"they\"),\n",
        "    (\"I entirely agree\", \"with\"),\n",
        "    (\"There cannot be a return to the status quo ante, and I\", \"think\"),\n",
        "    (\"The Commission came up with a very carefully crafted position on third countries investing in the EU, suggesting\", \"that\"),\n",
        "    (\"The answer, we are repeatedly told,\", \"is\"),\n",
        "]\n",
        "\n",
        "# INSTANTIATE AN OBJECT FROM FutureContextPredictor CLASS\n",
        "qwen_predictor = QwenFutureContextPredictor(\n",
        "    model_name= \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    device= \"cuda\",\n",
        "    is_instruction_tuned = True,\n",
        "    instruction_prefix= \"Predict the next token for the given incomplete text:\",\n",
        "    system_prompt= \"Your mission is the next-token prediction for a given incomplete sentence.\",\n",
        "    examples = examples\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lkwHcsDN5nF"
      },
      "source": [
        "**GIVE EXAMPLES TO THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUQshcZMOixN"
      },
      "outputs": [],
      "source": [
        "def predict(qwen_predictor, input_sentence, previous_context, num_of_tokens_to_predict):\n",
        "  if previous_context == \"\":\n",
        "    return qwen_predictor.predict(input_sentence, max_new_tokens=num_of_tokens_to_predict, with_examples = True)\n",
        "  else:\n",
        "    return qwen_predictor.predict(previous_context + \" \" + input_sentence, max_new_tokens=num_of_tokens_to_predict, with_examples = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mo_5n_nN-3H"
      },
      "source": [
        "**DO NOT GIVE ANY EXAMPLES TO THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pL5DlG-OvF7"
      },
      "outputs": [],
      "source": [
        "def predict(qwen_predictor, input_sentence, previous_context, num_of_tokens_to_predict):\n",
        "  if previous_context == \"\":\n",
        "    return qwen_predictor.predict(input_sentence, max_new_tokens=num_of_tokens_to_predict, with_examples = False)\n",
        "  else:\n",
        "    return qwen_predictor.predict(previous_context + \" \" + input_sentence, max_new_tokens=num_of_tokens_to_predict, with_examples = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fQANlftO4j4"
      },
      "source": [
        "**GENERATE PREDICTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeDmUurZOGxy"
      },
      "outputs": [],
      "source": [
        "# Take a specified amount of words from reference sentences as input at each step\n",
        "\n",
        "initial_word_amount_to_read = 30 # amount of words in the input for the first step\n",
        "last_word_amount_to_read = 30  # amount of words in the input for the last step\n",
        "word_amount_step = 5  # number by which the amount of words in the input increases\n",
        "num_of_tokens_to_predict = 4 # number of tokens which should be predicted by the model\n",
        "previous_context_max_size = 10 # maximum amount of previous sentences\n",
        "\n",
        "final_result_list = list() # store results of reference sentences as a list of list where each element is a list of results for a specific reference sentence\n",
        "\n",
        "for index, ref_sentence in enumerate(ref_transcriptions):\n",
        "  result_list = list() # store each result for a specific reference sentence as elements of a list\n",
        "  word_amount_to_read = initial_word_amount_to_read\n",
        "\n",
        "  while word_amount_to_read <= last_word_amount_to_read:\n",
        "    input_sentence = \" \".join(ref_sentence.split()[:word_amount_to_read])\n",
        "    print(\"Input sentence:\", input_sentence)\n",
        "    previous_context_start_index = 0 if (index < previous_context_max_size) else (index - previous_context_max_size)\n",
        "    previous_context = \" \".join(ref_transcriptions[previous_context_start_index:index])\n",
        "    prediction,output_dict = predict(qwen_predictor, input_sentence, previous_context, num_of_tokens_to_predict)\n",
        "    predicted_words = prediction.split()\n",
        "    predicted_token_ids = qwen_predictor.tokenizer([prediction], return_tensors=\"pt\").to(qwen_predictor.model.device)[\"input_ids\"][0]\n",
        "    result_dict = {\"word_amount_to_read\": word_amount_to_read,\n",
        "                   \"predicted_token_ids\": predicted_token_ids,\n",
        "                   \"predicted_words\": prediction.split(),\n",
        "                   \"input\": previous_context + \" \" + input_sentence,\n",
        "                   \"ground-truth_words\": ref_sentence.split()[word_amount_to_read:word_amount_to_read + len(predicted_words)],\n",
        "                   \"output_scores\": output_dict[\"scores\"]\n",
        "                   }\n",
        "    print(\"Result:\", result_dict)\n",
        "    print()\n",
        "    result_list.append(result_dict)\n",
        "    word_amount_to_read += word_amount_step\n",
        "\n",
        "  final_result_list.append(result_list)\n",
        "print(final_result_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_uYxdaaNhm8"
      },
      "source": [
        "**USE QWEN 2.5-0.5B BASE MODEL TO IMPLEMENT NEXT-WORD PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_0j296kFKn0"
      },
      "outputs": [],
      "source": [
        "# INSTANTIATE AN OBJECT FROM QwenFutureContextPredictor CLASS\n",
        "qwen_predictor = QwenFutureContextPredictor(\n",
        "    model_name= \"Qwen/Qwen2.5-0.5B\",\n",
        "    device= \"cuda\",\n",
        "    system_prompt= \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\",\n",
        "    is_instruction_tuned= False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlmEE75TF_b1"
      },
      "outputs": [],
      "source": [
        "example_input = \"I need to get some fresh hair, thus I will\"\n",
        "print(example_input)\n",
        "output,output_dict = qwen_predictor.predict(example_input, max_new_tokens =3, with_examples=False)\n",
        "\n",
        "print(\"******************\")\n",
        "print(\"Output:\", output)\n",
        "print(\"******************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jffAMhURq4C"
      },
      "outputs": [],
      "source": [
        "# Take a specified amount of words from reference sentences as input at each step\n",
        "\n",
        "initial_word_amount_to_read = 30 # amount of words in the input for the first step\n",
        "last_word_amount_to_read = 30  # amount of words in the input for the last step\n",
        "word_amount_step = 5  # number by which the amount of words in the input increases\n",
        "num_of_tokens_to_predict = 4 # number of tokens which should be predicted by the model\n",
        "with_examples = False\n",
        "\n",
        "final_result_list = list() # store results of reference sentences as a list of list where each element is a list of results for a specific reference sentence\n",
        "\n",
        "for ref_sentence in ref_transcriptions:\n",
        "  result_list = list() # store each result for a specific reference sentence as elements of a list\n",
        "  word_amount_to_read = initial_word_amount_to_read\n",
        "\n",
        "  while word_amount_to_read <= last_word_amount_to_read:\n",
        "    input_sentence = \" \".join(ref_sentence.split()[:word_amount_to_read])\n",
        "    print(\"Input sentence:\", input_sentence)\n",
        "    prediction,output_dict = qwen_predictor.predict(input_sentence, max_new_tokens=num_of_tokens_to_predict, with_examples=with_examples)\n",
        "    predicted_words = prediction.split()\n",
        "    print(\"Predicted text:\", prediction)\n",
        "    predicted_token_ids = qwen_predictor.tokenizer([prediction], return_tensors=\"pt\").to(qwen_predictor.model.device)[\"input_ids\"][0]\n",
        "    result_dict = {\n",
        "        \"word_amount_to_read\": word_amount_to_read,\n",
        "        \"predicted_token_ids\": predicted_token_ids,\n",
        "        \"predicted_words\": predicted_words,\n",
        "        \"input\": input_sentence,\n",
        "        \"ground-truth_words\": ref_sentence.split()[word_amount_to_read:word_amount_to_read + len(predicted_words)],\n",
        "        \"output_scores\": output_dict[\"scores\"]\n",
        "        }\n",
        "    print(\"Result:\", result_dict)\n",
        "    print()\n",
        "    result_list.append(result_dict)\n",
        "    word_amount_to_read += word_amount_step\n",
        "\n",
        "  final_result_list.append(result_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN3s7R2X4nOB"
      },
      "source": [
        "**CALCULATE ACCURACY WORD BY WORD - START**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4eW6gzB7WRh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def find_word_end_indexes(generated_tokens, num_of_words):\n",
        "  word_end_indexes = list()\n",
        "  token_index = 1\n",
        "  for index in range(num_of_words):\n",
        "    print(\"\\nWord Number:\", index+1)\n",
        "    word_ended = False\n",
        "    while (not word_ended) and token_index < len(generated_tokens):\n",
        "      token = generated_tokens[token_index]\n",
        "      print(\"Token:\", token)\n",
        "      if token[0] == \" \":\n",
        "        word_ended = True\n",
        "      token_index += 1\n",
        "    if word_ended:\n",
        "      word_end_index = token_index - 2\n",
        "    else:\n",
        "      word_end_index = None\n",
        "    word_end_indexes.append(word_end_index)\n",
        "  print(\"Word End Indexes:\", word_end_indexes)\n",
        "  return word_end_indexes\n",
        "\n",
        "def find_probability_of_words(word_end_indexes, generated_ids, output_dict):\n",
        "  word_probabilities = list()\n",
        "  start_index = 0\n",
        "  for end_index in word_end_indexes:\n",
        "    if end_index == None:\n",
        "      word_probabilities.append(0)\n",
        "      # no need to assign new value for start_index\n",
        "      continue\n",
        "    print(\"\\nStart Index:\", start_index)\n",
        "    token_probability_sum = 0\n",
        "    for token_index in range(start_index,end_index+1):\n",
        "      print(\"Token Index:\", token_index)\n",
        "      token_id = generated_ids[token_index]\n",
        "      print(\"Token id:\", token_id)\n",
        "      token_probability = torch.exp(torch.nn.functional.log_softmax(output_dict[\"scores\"][token_index][0], dim=-1)[token_id])\n",
        "      print(\"Token_probabilty:\", token_probability)\n",
        "      token_probability_sum += token_probability\n",
        "    token_num = end_index - start_index + 1\n",
        "    print(\"Token Number:\", token_num)\n",
        "    print(\"Token Probability Sum:\", token_probability_sum)\n",
        "    word_probability = token_probability_sum/token_num\n",
        "    word_probabilities.append(word_probability)\n",
        "    print(\"Word Probability:\", word_probability)\n",
        "    start_index = end_index + 1\n",
        "  return word_probabilities\n",
        "\n",
        "def predict(input_text, num_of_words_to_predict, previous_context, model, tokenizer, tokens_per_word = 4):\n",
        "  max_new_tokens = num_of_words_to_predict * tokens_per_word\n",
        "  if previous_context:\n",
        "    input = previous_context + \" \" + input_text\n",
        "  else:\n",
        "    input = input_text\n",
        "  print(\"Input to model:\", input)\n",
        "  model_inputs = tokenizer([input], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "  output_dict = model.generate(\n",
        "      **model_inputs,\n",
        "      max_new_tokens=max_new_tokens,\n",
        "      return_dict_in_generate=True,\n",
        "      output_scores=True\n",
        "      )\n",
        "  generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, output_dict[\"sequences\"])]\n",
        "  prediction = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  generated_tokens= tokenizer.batch_decode(generated_ids[0])\n",
        "  print(\"Generated Tokens:\", generated_tokens)\n",
        "  word_end_indexes = find_word_end_indexes(generated_tokens, num_of_words_to_predict)\n",
        "  word_probabilities = find_probability_of_words(word_end_indexes, generated_ids[0], output_dict)\n",
        "\n",
        "  print(\"*******************************************************************\")\n",
        "  return (prediction,word_probabilities)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZUn3GBh4mzG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\", torch_dtype=\"auto\", device_map=\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
        "previous_context_max_size = 0 # maximum amount of previous sentences\n",
        "correct_prediction_counter_by_threshold = {\"0.0\":0,\"0.4\":0, \"0.5\":0, \"0.6\":0, \"0.7\":0, \"0.8\":0}\n",
        "total_prediction_counter_by_threshold = {\"0.0\":0,\"0.4\":0, \"0.5\":0, \"0.6\":0, \"0.7\":0, \"0.8\":0}\n",
        "num_of_words_to_predict = 1\n",
        "thresholds = [0.0,0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for sentence_index,ref_sentence in enumerate(ref_transcriptions):\n",
        "  previous_context_start_index = 0 if (sentence_index <= previous_context_max_size) else (sentence_index - previous_context_max_size)\n",
        "  previous_context_sentences = ref_transcriptions[previous_context_start_index:sentence_index]\n",
        "  previous_context = \" \".join(previous_context_sentences)\n",
        "  print(\"\\nSentence\", sentence_index+1)\n",
        "  print(\"Previous context:\", previous_context)\n",
        "  print(\"Previous context length:\", len(previous_context_sentences))\n",
        "  words_in_ref = ref_sentence.split()\n",
        "  for word_index in range(len(words_in_ref)-1):\n",
        "    next_word_index = word_index + 1\n",
        "    ground_truth_next_word = words_in_ref[next_word_index]\n",
        "    input_text = \" \".join(words_in_ref[:next_word_index])\n",
        "    print(\"Input text:\", input_text)\n",
        "    prediction,word_probabilities = predict(input_text=input_text,\n",
        "                                     num_of_words_to_predict=num_of_words_to_predict,\n",
        "                                     previous_context=previous_context,\n",
        "                                     model=model,\n",
        "                                     tokenizer=tokenizer\n",
        "                                     )\n",
        "    predicted_next_word = prediction.split()[0]\n",
        "    word_probability = word_probabilities[0]\n",
        "    print(\"Word Probability:\", word_probability)\n",
        "    print(\"Ground truth next word:\", ground_truth_next_word)\n",
        "    print(\"Predicted next word:\", predicted_next_word)\n",
        "\n",
        "    # TO-DO: BİRDEN FAZLA THRESHOLD İÇİN KONTROL YAP\n",
        "    for threshold in thresholds:\n",
        "      if word_probability > threshold:\n",
        "        total_prediction_counter_by_threshold[str(threshold)] += 1\n",
        "        print(\"Hit?:\", predicted_next_word == ground_truth_next_word)\n",
        "        if predicted_next_word == ground_truth_next_word:\n",
        "          correct_prediction_counter_by_threshold[str(threshold)] += 1\n",
        "    print(\"\\n***************************************************\")\n",
        "\n",
        "accuracy_by_threshold = {}\n",
        "for threshold in thresholds:\n",
        "  accuracy = round(correct_prediction_counter_by_threshold[str(threshold)]/total_prediction_counter_by_threshold[str(threshold)],3) * 100\n",
        "  accuracy_by_threshold[str(threshold)] = accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB9yD29QTtgN"
      },
      "outputs": [],
      "source": [
        "for threshold in thresholds:\n",
        "  print(\"Threshold (%):\", threshold * 100)\n",
        "  print(\"Correct predictions:\", correct_prediction_counter_by_threshold[str(threshold)])\n",
        "  print(\"Total predictions:\", total_prediction_counter_by_threshold[str(threshold)])\n",
        "  print(\"Accuracy:\", accuracy_by_threshold[str(threshold)])\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYG-t7DU4svD"
      },
      "source": [
        "**CALCULATE ACCURACY WORD BY WORD - END**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXvrB4sMb3hM"
      },
      "source": [
        "**USE Llama-3.2-1B BASE MODEL TO IMPLEMENT FUTURE-CONTEXT-PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVnm2AGNeLy-"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRk1zSBhb27r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "class LlamaFutureContextPredictor():\n",
        "\n",
        "  def __init__(self, model_id, device):\n",
        "    self.pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=device,\n",
        "    )\n",
        "\n",
        "  def predict(self, partial_text):\n",
        "    return self.pipe(partial_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcujH8cpdTJa"
      },
      "outputs": [],
      "source": [
        "llama_predictor = LlamaFutureContextPredictor(\n",
        "    model_id = \"meta-llama/Llama-3.2-1B\",\n",
        "    device = \"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EVnIhqwdY8x"
      },
      "outputs": [],
      "source": [
        "partial_text = \"Today is a beatiful\"\n",
        "print(llama_predictor.predict(partial_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTiehGoAzNX6"
      },
      "source": [
        "**STORE OUTPUTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kU26s-a5499"
      },
      "outputs": [],
      "source": [
        "output_lines = list()\n",
        "\n",
        "for index_1, final_result in enumerate(final_result_list):\n",
        "  output_line = \"Sentence \" + str(index_1 + 1) + \"\\n\"\n",
        "  for index_2, result in enumerate(final_result):\n",
        "    output_line += \"\\nWord Amount to Read: \" + str(result[\"word_amount_to_read\"]) + \"\\n\"\n",
        "    output_line += \"Input: \" + result[\"input\"] + \"\\n\"\n",
        "    output_line += \"Prediction: \" + \" \".join(result[\"predicted_words\"]) + \"\\n\"\n",
        "    output_line += \"Ground-truth: \" + \" \".join(result[\"ground-truth_words\"]) + \"\\n\"\n",
        "  output_line += \"\\n***************************************************************\\n\"\n",
        "  output_lines.append(output_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x25UpXGi5wzU"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/ColabNotebooks/future-context prediction/output.txt\", \"w\") as file:\n",
        "  file.writelines(output_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKtj5MAr5O6z"
      },
      "source": [
        "**EVALUATE FUTURE CONTEXT PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM1F4jug5OIi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import statistics\n",
        "import torch\n",
        "\n",
        "def get_results_for_step(final_result_list, step):\n",
        "  return list(map(lambda x: x[step], final_result_list))\n",
        "\n",
        "def prediction_matches_ground_truth(predicted_words, ground_truth_words):\n",
        "  predicted_words_to_check = predicted_words[:len(ground_truth_words)]\n",
        "  print(\"Predicted words to check:\", predicted_words_to_check)\n",
        "  print(\"Ground truth wordsto check:\", ground_truth_words)\n",
        "  return all(pred_and_ground[0] == pred_and_ground[1] for pred_and_ground in zip(predicted_words_to_check, ground_truth_words))\n",
        "\n",
        "'''\n",
        "Calculates the percentage of correct step results.\n",
        "A step result for a sentence is correct only if all predicted tokens are correct.\n",
        "'''\n",
        "def calculate_success_rate(num_of_words_to_check, step_results, ref_transcriptions, tokenizer, predictor):\n",
        "  step_results_to_check_counter = 0\n",
        "  correct_step_results_num = 0\n",
        "  pred_probabilities_for_correct_results = list()\n",
        "  pred_probabilities_for_false_results = list()\n",
        "\n",
        "  for step_result, ref_sentence in zip(step_results, ref_transcriptions):\n",
        "    predicted_words_to_check = step_result[\"predicted_words\"][:num_of_words_to_check]\n",
        "    predicted_token_ids = step_result[\"predicted_token_ids\"]\n",
        "    predicted_tokens_num = len(predicted_token_ids)\n",
        "    ground_truth_words_to_check = step_result[\"ground-truth_words\"][:num_of_words_to_check]\n",
        "    #ground_truth_tokens = tokenizer([ref_sentence], return_tensors=\"pt\").to(predictor.model.device)[\"input_ids\"][0]\n",
        "    #ground_truth_next_token_ids = ground_truth_tokens[num_of_tokens_to_read:(num_of_tokens_to_read + predicted_tokens_num)]\n",
        "    print(\"Predicted tokens:\", tokenizer.batch_decode(predicted_token_ids))\n",
        "    print(\"Predicted token ids:\", predicted_token_ids)\n",
        "    print(\"Number of predicted token ids:\", len(predicted_token_ids))\n",
        "    print(\"Reference sentence:\", ref_sentence)\n",
        "    #print(\"Number of tokens in reference:\", len(ground_truth_tokens))\n",
        "    #print(\"Number of tokens read:\", num_of_tokens_to_read)\n",
        "    #print(\"Ground truth tokens:\", tokenizer.batch_decode(ground_truth_tokens,skip_special_tokens=True))\n",
        "    #print(\"Ground truth next tokens:\", tokenizer.batch_decode(ground_truth_next_token_ids))\n",
        "    #print(\"Ground truth next token ids:\", ground_truth_next_token_ids)\n",
        "\n",
        "    is_to_be_checked = ground_truth_words_to_check != []\n",
        "\n",
        "    if is_to_be_checked:\n",
        "      step_results_to_check_counter += 1\n",
        "      pred_probabilities = [torch.exp(torch.nn.functional.log_softmax(step_result[\"output_scores\"][0][0], dim=-1)[predicted_token_ids[i]]) for i in range(predicted_tokens_num)]\n",
        "\n",
        "    if prediction_matches_ground_truth(predicted_words_to_check, ground_truth_words_to_check) and is_to_be_checked:\n",
        "      pred_probabilities_for_correct_results.append(pred_probabilities)\n",
        "      correct_step_results_num += 1\n",
        "      print(\"Hit\")\n",
        "    elif is_to_be_checked:\n",
        "      pred_probabilities_for_false_results.append(pred_probabilities)\n",
        "      print(\"Miss\")\n",
        "    print()\n",
        "\n",
        "  print(len(pred_probabilities_for_correct_results) == correct_step_results_num)\n",
        "  print(\"Correct results:\", correct_step_results_num)\n",
        "  print(\"Step results to check:\", step_results_to_check_counter)\n",
        "\n",
        "  pred_probabilities = {\n",
        "      \"for_correct\": pred_probabilities_for_correct_results,\n",
        "      \"for_false\": pred_probabilities_for_false_results\n",
        "  }\n",
        "  success_rate = round(correct_step_results_num / step_results_to_check_counter,3) * 100\n",
        "  return (success_rate, pred_probabilities)\n",
        "\n",
        "def get_pred_probability_means_for_index(predicted_token_index, pred_probabilities_for_step):\n",
        "  pred_probabilities_for_correct_results = list(map(lambda x: x[predicted_token_index].item(), pred_probabilities_for_step[\"for_correct\"]))\n",
        "  pred_probabilities_for_false_results = list(map(lambda x: x[predicted_token_index].item(), pred_probabilities_for_step[\"for_false\"]))\n",
        "\n",
        "  return {\n",
        "      \"for_correct\": round(statistics.mean(pred_probabilities_for_correct_results), 2),\n",
        "      \"for_false\": round(statistics.mean(pred_probabilities_for_false_results), 2)\n",
        "  }\n",
        "\n",
        "predictor = qwen_predictor\n",
        "tokenizer = predictor.tokenizer\n",
        "predicted_token_index = 0\n",
        "word_amount_to_read = initial_word_amount_to_read\n",
        "step = 0\n",
        "num_of_words_to_check = 1\n",
        "success_rates_by_step = list()\n",
        "pred_probability_results= {\n",
        "    \"predicted_token_index\": predicted_token_index,\n",
        "    \"means_by_word_amount_to_read\": {},\n",
        "    }\n",
        "while word_amount_to_read <= last_word_amount_to_read:\n",
        "  print(\"****************************************************\")\n",
        "  print(\"Step \" + str(step) + \", Word Amount to Read:\", word_amount_to_read)\n",
        "  print(\"****************************************************\")\n",
        "  step_results = get_results_for_step(final_result_list, step)\n",
        "  print(\"Step results length:\", len(step_results))\n",
        "  success_rate, pred_probabilities_for_step = calculate_success_rate(num_of_words_to_check, step_results, ref_transcriptions, tokenizer, predictor)\n",
        "  #pred_probability_means = get_pred_probability_means_for_index(predicted_token_index, pred_probabilities_for_step)\n",
        "  print(\"Success rate for \" + str(word_amount_to_read) + \" of words in the reference sentences:\", success_rate)\n",
        "  print()\n",
        "  success_rates_by_step.append(success_rate)\n",
        "  #pred_probability_results[\"means_by_word_amount_to_read\"][word_amount_to_read] = pred_probability_means\n",
        "  step += 1\n",
        "  word_amount_to_read += word_amount_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYdpIcctosGR"
      },
      "outputs": [],
      "source": [
        "# Print Success Rate Results\n",
        "for index, success_rate in enumerate(success_rates_by_step):\n",
        "  word_amount_to_read = initial_word_amount_to_read + index * word_amount_step\n",
        "  print(\"Success rate for first \" + str(word_amount_to_read) + \" from the reference sentence:\", round(success_rate,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8HNmwapcnjF"
      },
      "outputs": [],
      "source": [
        "# Print Prediction Probability Mean Results\n",
        "means_by_percent = pred_probability_results[\"means_by_percent\"]\n",
        "print(\"Means of prediction probabilities for each step in % (predicted token index = \" + str(pred_probability_results[\"predicted_token_index\"]) + \"):\\n\\n\")\n",
        "for index in range(len(success_rates_by_step)):\n",
        "  percentage = start_percentage + index * percentage_step\n",
        "  mean_for_correct_sentences = means_by_percent[percentage][\"for_correct\"] * 100\n",
        "  mean_for_false_sentences = means_by_percent[percentage][\"for_false\"] * 100\n",
        "  print(\"Prediction probability mean for correct sentences with \" + str(round(percentage,2) * 100) + \" % of reference sentences as input:\", mean_for_correct_sentences)\n",
        "  print(\"Prediction probability mean for correct sentences with \" + str(round(percentage,2) * 100) + \" % of reference sentences as input:\", mean_for_false_sentences)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW-665yKf4ez"
      },
      "outputs": [],
      "source": [
        "# Print Prediction Probability Mean Results\n",
        "means_by_percent = pred_probability_results[\"means_by_percent\"]\n",
        "print(\"Means of prediction probabilities for each step in % (predicted token index = \" + str(pred_probability_results[\"predicted_token_index\"]) + \"):\\n\\n\")\n",
        "for index in range(len(success_rates_by_step)):\n",
        "  percentage = start_percentage + index * percentage_step\n",
        "  mean_for_correct_sentences = means_by_percent[percentage][\"for_correct\"] * 100\n",
        "  mean_for_false_sentences = means_by_percent[percentage][\"for_false\"] * 100\n",
        "  print(\"Prediction probability mean for correct sentences with \" + str(round(percentage,2) * 100) + \" % of reference sentences as input:\", mean_for_correct_sentences)\n",
        "  print(\"Prediction probability mean for correct sentences with \" + str(round(percentage,2) * 100) + \" % of reference sentences as input:\", mean_for_false_sentences)\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcNhXgVITzsykj262XzFJb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}